// Audio Manager Class - Dual Stream (System + Microphone)
class RendererAudioManager extends EventTarget {
  constructor() {
    super();
    this.isCapturing = false;
    this.isDeepgramConnected = false;
    
    // System audio (BlackHole) stream
    this.systemAudioContext = null;
    this.systemMediaStream = null;
    this.systemAudioLevel = 0;
    this.systemHasSignal = false;
    
    // Microphone stream
    this.micAudioContext = null;
    this.micMediaStream = null;
    this.micAudioLevel = 0;
    this.micHasSignal = false;
    
    // Combined/legacy properties for compatibility
    this.audioContext = null;
    this.mediaStream = null;
    this.scriptProcessor = null;
    this.audioLevel = 0;
    this.hasAudioSignal = false;
  }

  emit(eventName, data) {
    this.dispatchEvent(new CustomEvent(eventName, { detail: data }));
  }

  async initializeAudio() {
    return this.initializeAudioWithDevice();
  }

  async initializeAudioWithDevice(deviceId = null) {
    // This method is now simplified - the main audio setup happens in handleStartAudioCapture
    console.log('Audio manager initialized (stream will be set by handleStartAudioCapture)');
    return true;
  }

  async startSystemAudioCapture(systemStream) {
    try {
      console.log('DEBUG: startSystemAudioCapture() called');
      this.systemMediaStream = systemStream;
      
      if (!this.systemMediaStream) {
        console.error('DEBUG: No system mediaStream available');
        throw new Error('No system audio stream available');
      }

      console.log('DEBUG: System MediaStream tracks:', this.systemMediaStream.getTracks().length);
      console.log('DEBUG: System Audio tracks:', this.systemMediaStream.getAudioTracks().length);

      // Set up system audio level monitoring
      if (this.systemMediaStream.getAudioTracks().length > 0) {
        console.log('DEBUG: Setting up System AudioContext...');
        
        try {
          // Create AudioContext for system audio
          this.systemAudioContext = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: 16000
          });
          console.log('DEBUG: System AudioContext created, state:', this.systemAudioContext.state);
          console.log('DEBUG: System AudioContext sample rate:', this.systemAudioContext.sampleRate);
          
          const source = this.systemAudioContext.createMediaStreamSource(this.systemMediaStream);
          console.log('DEBUG: System MediaStreamSource created');
          
          const analyser = this.systemAudioContext.createAnalyser();
          analyser.fftSize = 512;
          analyser.minDecibels = -90;
          analyser.maxDecibels = -10;
          analyser.smoothingTimeConstant = 0.8;
          source.connect(analyser);
          console.log('DEBUG: System Analyser connected');
          
          // Set up system audio level monitoring
          const dataArray = new Uint8Array(analyser.frequencyBinCount);
          let frameCount = 0;
          
          const updateSystemLevel = () => {
            if (this.isCapturing) {
              analyser.getByteFrequencyData(dataArray);
              
              // Calculate RMS level
              let sum = 0;
              for (let i = 0; i < dataArray.length; i++) {
                sum += dataArray[i] * dataArray[i];
              }
              const rms = Math.sqrt(sum / dataArray.length);
              this.systemAudioLevel = Math.min(100, (rms / 255) * 100);
              
              // Debug every 60 frames (roughly once per second)
              if (frameCount % 60 === 0) {
                console.log('DEBUG: System Audio level:', this.systemAudioLevel.toFixed(2), 'RMS:', rms.toFixed(2));
              }
              frameCount++;
              
              // Detect signal presence
              const hasSignal = this.systemAudioLevel > 1;
              if (hasSignal !== this.systemHasSignal) {
                this.systemHasSignal = hasSignal;
                console.log('DEBUG: System Signal changed to:', hasSignal);
              }
              
              // Update combined levels
              this.updateCombinedLevels();
              
              requestAnimationFrame(updateSystemLevel);
            }
          };
          
          // Resume AudioContext if needed
          if (this.systemAudioContext.state === 'suspended') {
            console.log('DEBUG: Resuming suspended System AudioContext...');
            await this.systemAudioContext.resume();
          }
          
          // Set capturing flag BEFORE starting the loop
          this.isCapturing = true;
          
          updateSystemLevel();
          console.log('DEBUG: System Level monitoring started');
          
        } catch (audioError) {
          console.error('DEBUG: System AudioContext setup failed:', audioError);
          throw audioError;
        }
        
        this.systemHasSignal = true;
        this.emit('audio-signal-changed', true);
      } else {
        console.error('DEBUG: No system audio tracks found in stream');
      }
      
      console.log('DEBUG: System audio capture completed successfully');
      return true;
    } catch (error) {
      console.error('DEBUG: startSystemAudioCapture failed:', error);
      return false;
    }
  }

  async startMicrophoneCapture() {
    try {
      console.log('DEBUG: startMicrophoneCapture() called');
      
      // Get microphone device
      const devices = await navigator.mediaDevices.enumerateDevices();
      const micDevice = devices.find(device =>
        device.kind === 'audioinput' && 
        (device.label.toLowerCase().includes('macbook pro microphone') || 
         device.label.toLowerCase().includes('built-in'))
      );
      
      if (!micDevice) {
        console.error('DEBUG: Built-in microphone not found');
        throw new Error('Built-in microphone not found');
      }
      
      console.log('DEBUG: Found microphone device:', micDevice.label);
      
      // Request access to microphone
      const constraints = {
        audio: {
          deviceId: { exact: micDevice.deviceId },
          sampleRate: 16000,
          channelCount: 1,  // Mono for microphone
          echoCancellation: true,  // Enable for microphone
          noiseSuppression: true,
          autoGainControl: true
        },
        video: false
      };
      
      this.micMediaStream = await navigator.mediaDevices.getUserMedia(constraints);
      console.log('DEBUG: Successfully captured microphone stream!');
      console.log('DEBUG: Mic stream settings:', {
        sampleRate: this.micMediaStream.getAudioTracks()[0].getSettings().sampleRate,
        channelCount: this.micMediaStream.getAudioTracks()[0].getSettings().channelCount
      });
      
      // Set up microphone audio level monitoring
      if (this.micMediaStream.getAudioTracks().length > 0) {
        console.log('DEBUG: Setting up Microphone AudioContext...');
        
        try {
          // Create AudioContext for microphone
          this.micAudioContext = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: 16000
          });
          console.log('DEBUG: Mic AudioContext created, state:', this.micAudioContext.state);
          
          const source = this.micAudioContext.createMediaStreamSource(this.micMediaStream);
          console.log('DEBUG: Mic MediaStreamSource created');
          
          const analyser = this.micAudioContext.createAnalyser();
          analyser.fftSize = 512;
          analyser.minDecibels = -90;
          analyser.maxDecibels = -10;
          analyser.smoothingTimeConstant = 0.8;
          source.connect(analyser);
          console.log('DEBUG: Mic Analyser connected');
          
          // Set up microphone level monitoring
          const dataArray = new Uint8Array(analyser.frequencyBinCount);
          let frameCount = 0;
          
          const updateMicLevel = () => {
            if (this.isCapturing) {
              analyser.getByteFrequencyData(dataArray);
              
              // Calculate RMS level
              let sum = 0;
              for (let i = 0; i < dataArray.length; i++) {
                sum += dataArray[i] * dataArray[i];
              }
              const rms = Math.sqrt(sum / dataArray.length);
              this.micAudioLevel = Math.min(100, (rms / 255) * 100);
              
              // Debug every 60 frames (roughly once per second)
              if (frameCount % 60 === 0) {
                console.log('DEBUG: Mic Audio level:', this.micAudioLevel.toFixed(2), 'RMS:', rms.toFixed(2));
              }
              frameCount++;
              
              // Detect signal presence
              const hasSignal = this.micAudioLevel > 1;
              if (hasSignal !== this.micHasSignal) {
                this.micHasSignal = hasSignal;
                console.log('DEBUG: Mic Signal changed to:', hasSignal);
              }
              
              // Update combined levels
              this.updateCombinedLevels();
              
              requestAnimationFrame(updateMicLevel);
            }
          };
          
          // Resume AudioContext if needed
          if (this.micAudioContext.state === 'suspended') {
            console.log('DEBUG: Resuming suspended Mic AudioContext...');
            await this.micAudioContext.resume();
          }
          
          updateMicLevel();
          console.log('DEBUG: Mic Level monitoring started');
          
        } catch (audioError) {
          console.error('DEBUG: Mic AudioContext setup failed:', audioError);
          throw audioError;
        }
      }
      
      console.log('DEBUG: Microphone capture completed successfully');
      return true;
    } catch (error) {
      console.error('DEBUG: startMicrophoneCapture failed:', error);
      return false;
    }
  }

  updateCombinedLevels() {
    // Combine system and microphone levels
    this.audioLevel = Math.max(this.systemAudioLevel, this.micAudioLevel);
    this.hasAudioSignal = this.systemHasSignal || this.micHasSignal;
    
    // Emit combined signal change events
    this.emit('audio-signal-changed', this.hasAudioSignal);
  }

  async startAudioCapture() {
    try {
      console.log('DEBUG: startAudioCapture() called (legacy compatibility)');
      console.log('DEBUG: this.mediaStream exists?', !!this.mediaStream);
      
      if (!this.mediaStream) {
        console.error('DEBUG: No mediaStream available');
        throw new Error('No audio stream available');
      }

      console.log('DEBUG: MediaStream tracks:', this.mediaStream.getTracks().length);
      console.log('DEBUG: Audio tracks:', this.mediaStream.getAudioTracks().length);

      // Set up basic audio level monitoring
      if (this.mediaStream.getAudioTracks().length > 0) {
        console.log('DEBUG: Setting up AudioContext...');
        
        try {
          // Create minimal AudioContext with 16kHz sample rate to match stream
          this.audioContext = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: 16000
          });
          console.log('DEBUG: AudioContext created, state:', this.audioContext.state);
          console.log('DEBUG: AudioContext sample rate:', this.audioContext.sampleRate);
          
          const source = this.audioContext.createMediaStreamSource(this.mediaStream);
          console.log('DEBUG: MediaStreamSource created');
          
          // Create analyser for level detection (lightweight)
          const analyser = this.audioContext.createAnalyser();
          analyser.fftSize = 256;
          source.connect(analyser);
          console.log('DEBUG: Analyser connected');
          
          // Set up level monitoring
          const dataArray = new Uint8Array(analyser.frequencyBinCount);
          let frameCount = 0;
          
          const updateLevel = () => {
            if (this.isCapturing) {
              analyser.getByteFrequencyData(dataArray);
              
              // Calculate RMS level
              let sum = 0;
              for (let i = 0; i < dataArray.length; i++) {
                sum += dataArray[i] * dataArray[i];
              }
              const rms = Math.sqrt(sum / dataArray.length);
              this.audioLevel = Math.min(100, (rms / 255) * 100);
              
              // Debug every 60 frames (roughly once per second)
              if (frameCount % 60 === 0) {
                console.log('DEBUG: Audio level:', this.audioLevel.toFixed(2), 'RMS:', rms.toFixed(2));
              }
              frameCount++;
              
              // Detect signal presence
              const hasSignal = this.audioLevel > 1; // Low threshold for signal detection
              if (hasSignal !== this.hasAudioSignal) {
                this.hasAudioSignal = hasSignal;
                this.emit('audio-signal-changed', hasSignal);
                console.log('DEBUG: Signal changed to:', hasSignal);
              }
              
              requestAnimationFrame(updateLevel);
            }
          };
          
          // Resume AudioContext if needed
          if (this.audioContext.state === 'suspended') {
            console.log('DEBUG: Resuming suspended AudioContext...');
            await this.audioContext.resume();
          }
          
          // Set capturing flag BEFORE starting the loop
          this.isCapturing = true;
          
          updateLevel();
          console.log('DEBUG: Level monitoring started');
          
        } catch (audioError) {
          console.error('DEBUG: AudioContext setup failed:', audioError);
          throw audioError;
        }
        
        this.hasAudioSignal = true;
        this.emit('audio-signal-changed', true);
      } else {
        console.error('DEBUG: No audio tracks found in stream');
      }
      console.log('DEBUG: Audio capture with level monitoring completed successfully');
      return true;
    } catch (error) {
      console.error('DEBUG: startAudioCapture failed:', error);
      return false;
    }
  }

  async stopAudioCapture() {
    try {
      console.log('Stopping dual audio capture...');
      
      this.isCapturing = false;
      
      // Stop system audio stream
      if (this.systemMediaStream) {
        this.systemMediaStream.getTracks().forEach(track => track.stop());
        this.systemMediaStream = null;
        console.log('System audio stream stopped and cleared');
      }
      
      // Stop microphone stream
      if (this.micMediaStream) {
        this.micMediaStream.getTracks().forEach(track => track.stop());
        this.micMediaStream = null;
        console.log('Microphone stream stopped and cleared');
      }
      
      // Stop system audio context
      if (this.systemAudioContext) {
        await this.systemAudioContext.close();
        this.systemAudioContext = null;
        console.log('System audio context closed');
      }
      
      // Stop microphone audio context
      if (this.micAudioContext) {
        await this.micAudioContext.close();
        this.micAudioContext = null;
        console.log('Microphone audio context closed');
      }
      
      // Stop legacy stream (for compatibility)
      if (this.mediaStream) {
        this.mediaStream.getTracks().forEach(track => track.stop());
        this.mediaStream = null;
        console.log('Legacy audio stream stopped and cleared');
      }
      
      // Stop legacy audio context (for compatibility)
      if (this.audioContext) {
        await this.audioContext.close();
        this.audioContext = null;
        console.log('Legacy audio context closed');
      }
      
      // Reset all levels
      this.systemAudioLevel = 0;
      this.systemHasSignal = false;
      this.micAudioLevel = 0;
      this.micHasSignal = false;
      this.audioLevel = 0;
      this.hasAudioSignal = false;
      
      this.emit('audio-signal-changed', false);
      
      console.log('Dual audio capture stopped successfully');
      return true;
    } catch (error) {
      console.error('Error stopping dual audio capture:', error);
      return false;
    }
  }

  async getAudioStatus() {
    try {
      const status = await window.electronAPI.getAudioStatus();
      this.isCapturing = status.isCapturing;
      this.isDeepgramConnected = status.isDeepgramConnected;
      this.hasAudioSignal = status.hasAudioSignal;
      this.audioLevel = status.audioLevel;
      return status;
    } catch (error) {
      console.error('Error getting audio status:', error);
      return { 
        isCapturing: false, 
        isDeepgramConnected: false,
        hasAudioSignal: false,
        audioLevel: 0
      };
    }
  }

  isCapturingAudio() {
    return this.isCapturing;
  }

  isDeepgramConnected() {
    return this.isDeepgramConnected;
  }

  hasAudioSignalDetected() {
    return this.hasAudioSignal;
  }

  getAudioLevel() {
    return this.audioLevel;
  }
}

// Export for use in renderer
window.RendererAudioManager = RendererAudioManager;

// Simple global variables
let isAuthenticated = false;
let isAudioCapturing = false;
let isDeepgramConnected = false;
let currentSession = null;
let websocketConnected = false;
let audioManager = null;

// Audio streaming to Heroku will be implemented here

// Placeholder for future Heroku audio streaming class
class HerokuAudioStreamer {
    constructor() {
        this.isStreaming = false;
        this.transcripts = [];
    }
    
    // Placeholder methods - will be implemented when we add Heroku streaming
    startStreaming() {
        console.log('HerokuAudioStreamer: Starting audio stream to Heroku...');
        this.isStreaming = true;
        return true;
    }
    
    stopStreaming() {
        console.log('HerokuAudioStreamer: Stopping audio stream to Heroku...');
        this.isStreaming = false;
        return true;
    }
    
    clearTranscripts() {
        this.transcripts = [];
        const transcriptDisplay = document.getElementById('transcript-display');
        if (transcriptDisplay) {
            transcriptDisplay.innerHTML = '<div class="transcript-placeholder">Transcripts will appear here when you start transcription...</div>';
        }
    }
}

// Placeholder for removed Deepgram methods - will be replaced with Heroku streaming
            } else {
                console.error('DeepgramManager: Connection failed:', result.error);
                this.updateConnectionStatus('✗ Connection failed', 'disconnected');
                return false;
            }
        } catch (error) {
            console.error('DeepgramManager: Connection error:', error);
            this.updateConnectionStatus('✗ Connection error', 'disconnected');
            return false;
        }
    }

    async disconnect() {
        try {
            console.log('DeepgramManager: Disconnecting...');
            await this.stopStreaming();
            
            const result = await window.electronAPI.deepgramDisconnect();
            this.isConnected = false;
            this.updateConnectionStatus('✗ Disconnected', 'disconnected');
            this.enableTranscriptionControls(false);
            
            console.log('DeepgramManager: Disconnected');
            return true;
        } catch (error) {
            console.error('DeepgramManager: Disconnect error:', error);
            return false;
        }
    }

    async startStreaming() {
        try {
            if (!this.isConnected) {
                console.error('DeepgramManager: Cannot start streaming - not connected');
                return false;
            }

            console.log('DeepgramManager: Starting streaming...');
            const result = await window.electronAPI.deepgramStartStreaming();
            
            if (result.success) {
                this.isStreaming = true;
                this.updateTranscriptionStatus('🎙️ Recording', 'streaming');
                this.startAudioStreaming();
                console.log('DeepgramManager: Streaming started');
                return true;
            } else {
                console.error('DeepgramManager: Failed to start streaming:', result.error);
                return false;
            }
        } catch (error) {
            console.error('DeepgramManager: Streaming start error:', error);
            return false;
        }
    }

    async stopStreaming() {
        try {
            console.log('DeepgramManager: Stopping streaming...');
            
            this.stopAudioStreaming();
            
            if (this.isConnected) {
                const result = await window.electronAPI.deepgramStopStreaming();
                console.log('DeepgramManager: Stop streaming result:', result);
            }
            
            this.isStreaming = false;
            this.updateTranscriptionStatus('⏹️ Stopped', 'stopped');
            console.log('DeepgramManager: Streaming stopped');
            return true;
        } catch (error) {
            console.error('DeepgramManager: Streaming stop error:', error);
            return false;
        }
    }

    startAudioStreaming() {
        // Start MediaRecorder to capture audio for Deepgram
        this.setupMediaRecorder();
    }

    stopAudioStreaming() {
        if (this.mediaRecorder && this.mediaRecorder.state !== 'inactive') {
            this.mediaRecorder.stop();
            this.mediaRecorder = null;
        }
        if (this.audioContext && this.audioContext.state !== 'closed') {
            this.audioContext.close();
            this.audioContext = null;
        }
    }

    setupMediaRecorder() {
        try {
            if (!audioManager || !audioManager.systemMediaStream || !audioManager.micMediaStream) {
                console.error('DeepgramManager: Audio streams not available');
                return;
            }

            // Create a combined stream from both system and microphone
            const audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
            
            // Create sources from both streams
            const systemSource = audioContext.createMediaStreamSource(audioManager.systemMediaStream);
            const micSource = audioContext.createMediaStreamSource(audioManager.micMediaStream);
            
            // Create a gain node to mix the streams
            const mixerGain = audioContext.createGain();
            mixerGain.gain.value = 1.0;
            
            // Connect both sources to the mixer
            systemSource.connect(mixerGain);
            micSource.connect(mixerGain);
            
            // Use MediaRecorder like the GitHub example (simpler and more stable)
            const destination = audioContext.createMediaStreamDestination();
            mixerGain.connect(destination);
            
            // Create MediaRecorder with the mixed stream
            this.mediaRecorder = new MediaRecorder(destination.stream, {
                mimeType: 'audio/webm;codecs=opus'
            });

            this.mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0 && this.isStreaming) {
                    console.log(`DeepgramManager: Sending ${event.data.size} bytes to Deepgram`);
                    this.sendAudioBlobToDeepgram(event.data);
                }
            };

            this.mediaRecorder.onerror = (event) => {
                console.error('DeepgramManager: MediaRecorder error:', event.error);
            };

            // Start recording with small chunks for real-time
            this.mediaRecorder.start(100); // 100ms chunks
            
            // Store references for cleanup
            this.audioContext = audioContext;
            
            console.log('DeepgramManager: Audio processor started');

        } catch (error) {
            console.error('DeepgramManager: Error setting up MediaRecorder:', error);
        }
    }

    async sendAudioBlobToDeepgram(audioBlob) {
        try {
            // Convert blob to ArrayBuffer (like the GitHub example)
            const arrayBuffer = await audioBlob.arrayBuffer();
            
            // Send to main process for Deepgram
            const result = await window.electronAPI.deepgramSendAudio(arrayBuffer);
            if (!result.success) {
                console.warn('DeepgramManager: Failed to send audio blob:', result.error);
            } else {
                console.log('DeepgramManager: Audio blob sent successfully');
            }
        } catch (error) {
            console.error('DeepgramManager: Error sending audio blob:', error);
        }
    }

    addTranscript(transcript) {
        const timestamp = new Date().toLocaleTimeString();
        const transcriptData = {
            timestamp: timestamp,
            text: transcript.text,
            confidence: transcript.confidence || 0,
            id: Date.now()
        };

        this.transcripts.push(transcriptData);
        this.displayTranscript(transcriptData);
        
        // Keep only last 50 transcripts to prevent memory issues
        if (this.transcripts.length > 50) {
            this.transcripts = this.transcripts.slice(-50);
        }
    }

    displayTranscript(transcriptData) {
        const transcriptDisplay = document.getElementById('transcript-display');
        const placeholder = transcriptDisplay.querySelector('.transcript-placeholder');
        
        // Remove placeholder if it exists
        if (placeholder) {
            placeholder.remove();
        }

        // Create transcript line
        const transcriptLine = document.createElement('div');
        transcriptLine.className = 'transcript-line';
        transcriptLine.innerHTML = `
            <span class="transcript-timestamp">[${transcriptData.timestamp}]</span>
            <span class="transcript-text">${transcriptData.text}</span>
        `;

        transcriptDisplay.appendChild(transcriptLine);
        
        // Auto-scroll to bottom
        transcriptDisplay.scrollTop = transcriptDisplay.scrollHeight;
    }

    clearTranscripts() {
        this.transcripts = [];
        const transcriptDisplay = document.getElementById('transcript-display');
        transcriptDisplay.innerHTML = `
            <div class="transcript-placeholder">
                Transcripts will appear here when you start transcription...
            </div>
        `;
    }

    updateConnectionStatus(text, className) {
        const statusElement = document.getElementById('deepgram-connection-status');
        if (statusElement) {
            statusElement.textContent = text;
            statusElement.className = `status-text ${className}`;
        }
    }

    updateTranscriptionStatus(text, className) {
        const statusElement = document.getElementById('transcription-status');
        if (statusElement) {
            statusElement.textContent = text;
            statusElement.className = `status-text ${className}`;
        }
    }

    enableTranscriptionControls(enabled) {
        const startBtn = document.getElementById('transcription-start-btn');
        const stopBtn = document.getElementById('transcription-stop-btn');
        
        if (startBtn) startBtn.disabled = !enabled;
        if (stopBtn) stopBtn.disabled = !enabled;
    }
}

// Global audio streamer instance (will stream to Heroku)
let audioStreamer = null;

// Global error handler to catch any unhandled errors
window.addEventListener('error', (event) => {
    console.error('Global error caught:', event.error);
    console.error('Error details:', {
        message: event.message,
        filename: event.filename,
        lineno: event.lineno,
        colno: event.colno,
        error: event.error
    });
    
    // Show error on screen instead of white screen
    document.body.innerHTML = `
        <div style="padding: 20px; text-align: center; font-family: Arial, sans-serif; background: white; min-height: 100vh;">
            <h2>JavaScript Error Detected</h2>
            <p><strong>Error:</strong> ${event.message}</p>
            <p><strong>File:</strong> ${event.filename}</p>
            <p><strong>Line:</strong> ${event.lineno}</p>
            <p><strong>Column:</strong> ${event.colno}</p>
            <p>Please check the console for more details.</p>
        </div>
    `;
});

// Unhandled promise rejection handler
window.addEventListener('unhandledrejection', (event) => {
    console.error('Unhandled promise rejection:', event.reason);
    
    document.body.innerHTML = `
        <div style="padding: 20px; text-align: center; font-family: Arial, sans-serif; background: white; min-height: 100vh;">
            <h2>Unhandled Promise Rejection</h2>
            <p><strong>Error:</strong> ${event.reason}</p>
            <p>Please check the console for more details.</p>
        </div>
    `;
});

// Initialize when DOM is ready
document.addEventListener('DOMContentLoaded', () => {
    console.log('DOM loaded, setting up simple interface');
    
    try {
        // Check if electronAPI is available
        if (typeof window.electronAPI === 'undefined') {
            console.error('electronAPI is not available! This will cause a white screen.');
            document.body.innerHTML = `
                <div style="padding: 20px; text-align: center; font-family: Arial, sans-serif; background: white; min-height: 100vh;">
                    <h2>Error: Electron API Not Available</h2>
                    <p>The application cannot start because the Electron API is not loaded.</p>
                    <p>This usually means there's an issue with the preload script.</p>
                    <p>Please check the console for more details.</p>
                    <p>Available window properties: ${Object.keys(window).join(', ')}</p>
                </div>
            `;
            return;
        }
        
        console.log('electronAPI is available:', Object.keys(window.electronAPI));
        
        // Test a simple API call
        try {
            console.log('Testing electronAPI.checkAuthStatus...');
            const testResult = window.electronAPI.checkAuthStatus();
            console.log('API test result:', testResult);
        } catch (apiError) {
            console.error('API test failed:', apiError);
            document.body.innerHTML = `
                <div style="padding: 20px; text-align: center; font-family: Arial, sans-serif; background: white; min-height: 100vh;">
                    <h2>Error: Electron API Test Failed</h2>
                    <p>Failed to call electronAPI.checkAuthStatus(): ${apiError.message}</p>
                    <p>This indicates the preload script is not working correctly.</p>
                    <p>Please check the console for more details.</p>
                </div>
            `;
            return;
        }
        
        setupSimpleButtons();
        checkInitialStatus();
        setupWebSocketEvents();
        setupAudioEvents();
        
        // Initialize audio manager (but don't initialize audio yet)
        audioManager = new RendererAudioManager();
        console.log('Audio manager created (audio will be initialized when needed)');
        
        // Don't initialize audio automatically - wait for user action
        
        console.log('Initialization completed successfully');
        
    } catch (error) {
        console.error('Error during initialization:', error);
        console.error('Error stack:', error.stack);
        document.body.innerHTML = `
            <div style="padding: 20px; text-align: center; font-family: Arial, sans-serif; background: white; min-height: 100vh;">
                <h2>Error During Initialization</h2>
                <p><strong>Error:</strong> ${error.message}</p>
                <p><strong>Stack:</strong> ${error.stack}</p>
                <p>Please check the console for more details.</p>
            </div>
        `;
    }
});

// Simple button setup
function setupSimpleButtons() {
    console.log('Setting up simple buttons');
    
    // Show all sections initially
    showSection('auth-buttons');
    showSection('audio-controls-section');
    showSection('ready-section');
    
    // Get button references
    const salesforceBtn = document.getElementById('salesforce-btn');
    const slackBtn = document.getElementById('slack-btn');
    const startSessionBtn = document.getElementById('start-session-btn');
    const endSessionBtn = document.getElementById('end-session-btn');
    const audioCaptureBtn = document.getElementById('audio-capture-btn');
    const audioStopBtn = document.getElementById('audio-stop-btn');
    
    console.log('Found buttons:', {
        salesforce: !!salesforceBtn,
        slack: !!slackBtn,
        startSession: !!startSessionBtn,
        endSession: !!endSessionBtn,
        audioCapture: !!audioCaptureBtn,
        audioStop: !!audioStopBtn
    });
    
    // Debug: Check all buttons in document
    const allButtons = document.querySelectorAll('button');
    console.log('All buttons in document:', allButtons.length);
    allButtons.forEach((btn, index) => {
        console.log(`Button ${index}:`, btn.id, btn.textContent.trim(), 'display:', window.getComputedStyle(btn).display);
    });
    
    // Salesforce button
    if (salesforceBtn) {
        salesforceBtn.addEventListener('click', (event) => {
            console.log('Salesforce button clicked!', event);
            event.preventDefault();
            event.stopPropagation();
            handleSalesforceAuth();
        });
        console.log('Salesforce button handler attached');
        
        // Test if button is clickable
        salesforceBtn.style.cursor = 'pointer';
        salesforceBtn.onmouseover = () => console.log('Salesforce button hovered');
    } else {
        console.error('Salesforce button not found');
    }
    
    // Slack button
    if (slackBtn) {
        slackBtn.addEventListener('click', (event) => {
            console.log('Slack button clicked!', event);
            event.preventDefault();
            event.stopPropagation();
            handleSlackAuth();
        });
        console.log('Slack button handler attached');
        
        // Test if button is clickable
        slackBtn.style.cursor = 'pointer';
        slackBtn.onmouseover = () => console.log('Slack button hovered');
    } else {
        console.error('Slack button not found');
    }
    
    // Start session button
    if (startSessionBtn) {
        startSessionBtn.addEventListener('click', (event) => {
            console.log('Start session button clicked!', event);
            event.preventDefault();
            event.stopPropagation();
            handleStartSession();
        });
        console.log('Start session button handler attached');
        
        // Test if button is clickable
        startSessionBtn.style.cursor = 'pointer';
        startSessionBtn.onmouseover = () => console.log('Start session button hovered');
    } else {
        console.error('Start session button not found');
    }
    
    // End session button
    if (endSessionBtn) {
        endSessionBtn.addEventListener('click', (event) => {
            console.log('End session button clicked!', event);
            event.preventDefault();
            event.stopPropagation();
            handleEndSession();
        });
        console.log('End session button handler attached');
        
        // Test if button is clickable
        endSessionBtn.style.cursor = 'pointer';
        endSessionBtn.onmouseover = () => console.log('End session button hovered');
    } else {
        console.error('End session button not found');
    }
    
    // Audio capture button
    if (audioCaptureBtn) {
        audioCaptureBtn.addEventListener('click', (event) => {
            console.log('Audio capture button clicked!', event);
            event.preventDefault();
            event.stopPropagation();
            handleStartAudioCapture();
        });
        console.log('Audio capture button handler attached');
        
        // Test if button is clickable
        audioCaptureBtn.style.cursor = 'pointer';
        audioCaptureBtn.onmouseover = () => console.log('Audio capture button hovered');
    } else {
        console.error('Audio capture button not found');
    }
    
    // Audio stop button
    if (audioStopBtn) {
        audioStopBtn.addEventListener('click', (event) => {
            console.log('Audio stop button clicked!', event);
            event.preventDefault();
            event.stopPropagation();
            handleStopAudioCapture();
        });
        console.log('Audio stop button handler attached');
        
        // Test if button is clickable
        audioStopBtn.style.cursor = 'pointer';
        audioStopBtn.onmouseover = () => console.log('Audio stop button hovered');
    } else {
        console.error('Audio stop button not found');
    }
    
    // Setup Deepgram button handlers
    setupDeepgramButtons();
}

// Deepgram button setup (isolated from existing audio)
function setupDeepgramButtons() {
    console.log('Setting up Deepgram buttons');
    
    // Get Deepgram button references
    const deepgramConnectBtn = document.getElementById('deepgram-connect-btn');
    const deepgramDisconnectBtn = document.getElementById('deepgram-disconnect-btn');
    const transcriptionStartBtn = document.getElementById('transcription-start-btn');
    const transcriptionStopBtn = document.getElementById('transcription-stop-btn');
    const clearTranscriptBtn = document.getElementById('clear-transcript-btn');
    
    // Deepgram connect button
    if (deepgramConnectBtn) {
        deepgramConnectBtn.addEventListener('click', async (event) => {
            console.log('Deepgram connect button clicked!', event);
            event.preventDefault();
            event.stopPropagation();
            await handleDeepgramConnect();
        });
        console.log('Deepgram connect button handler attached');
    }
    
    // Deepgram disconnect button
    if (deepgramDisconnectBtn) {
        deepgramDisconnectBtn.addEventListener('click', async (event) => {
            console.log('Deepgram disconnect button clicked!', event);
            event.preventDefault();
            event.stopPropagation();
            await handleDeepgramDisconnect();
        });
        console.log('Deepgram disconnect button handler attached');
    }
    
    // Transcription start button
    if (transcriptionStartBtn) {
        transcriptionStartBtn.addEventListener('click', async (event) => {
            console.log('Transcription start button clicked!', event);
            event.preventDefault();
            event.stopPropagation();
            await handleTranscriptionStart();
        });
        console.log('Transcription start button handler attached');
    }
    
    // Transcription stop button
    if (transcriptionStopBtn) {
        transcriptionStopBtn.addEventListener('click', async (event) => {
            console.log('Transcription stop button clicked!', event);
            event.preventDefault();
            event.stopPropagation();
            await handleTranscriptionStop();
        });
        console.log('Transcription stop button handler attached');
    }
    
    // Clear transcript button
    if (clearTranscriptBtn) {
        clearTranscriptBtn.addEventListener('click', (event) => {
            console.log('Clear transcript button clicked!', event);
            event.preventDefault();
            event.stopPropagation();
            handleClearTranscript();
        });
        console.log('Clear transcript button handler attached');
    }
}

// WebSocket event setup
function setupWebSocketEvents() {
    // Listen for WebSocket events from main process
    window.electronAPI.onWebSocketConnected(() => {
        console.log('WebSocket connected');
        websocketConnected = true;
        updateStatus('Connected to Heroku backend', 'success');
    });
    
    window.electronAPI.onWebSocketDisconnected(() => {
        console.log('WebSocket disconnected');
        websocketConnected = false;
        updateStatus('Disconnected from Heroku backend', 'warning');
    });
    
    window.electronAPI.onWebSocketError((error) => {
        console.error('WebSocket error:', error);
        updateStatus('WebSocket error: ' + error, 'error');
    });
    
    window.electronAPI.onSessionCreated(async (response) => {
        console.log('Session created event received:', response);
        console.log('Response type:', typeof response);
        console.log('Response keys:', response ? Object.keys(response) : 'no keys');
        
        // The response structure from Heroku is: { success: true, session: { session_id: "...", ... } }
        let session = response;
        let sessionId = 'unknown';
        
        // If this is the full response object with success/session fields
        if (response && response.session) {
            console.log('Found session object in response.session');
            session = response.session;
            currentSession = session;
        } else {
            console.log('Using response directly as session object');
            currentSession = response;
        }
        
        console.log('Session object:', session);
        console.log('Session object keys:', session ? Object.keys(session) : 'no keys');
        
        // Handle different session ID structures
        if (session && session.session_id) {
            sessionId = session.session_id;
            console.log('Found sessionId in session.session_id:', sessionId);
        } else if (session && session.id) {
            sessionId = session.id;
            console.log('Found sessionId in session.id:', sessionId);
        } else if (typeof session === 'string') {
            sessionId = session;
            console.log('Found sessionId as string:', sessionId);
        } else if (session && session.sessionId) {
            sessionId = session.sessionId;
            console.log('Found sessionId in session.sessionId:', sessionId);
        } else {
            console.error('Could not find session ID in any expected field');
            console.error('Session structure:', JSON.stringify(session, null, 2));
        }
        
        console.log('Extracted session ID:', sessionId);
        updateStatus('Session created! ID: ' + sessionId, 'success');
        
        // Now start the session automatically
        if (sessionId && sessionId !== 'unknown') {
            console.log('Auto-starting session with ID:', sessionId);
            try {
                const startResult = await window.electronAPI.startSession(sessionId);
                console.log('Auto-start session result:', startResult);
                if (startResult.success) {
                    updateStatus('Session started successfully!', 'success');
                } else {
                    updateStatus('Failed to start session: ' + startResult.error, 'error');
                }
            } catch (error) {
                console.error('Error auto-starting session:', error);
                updateStatus('Error starting session: ' + error.message, 'error');
            }
        }
        
        // Show end session button
        updateSessionButton('End Session');
    });
    
    window.electronAPI.onSessionStarted((session) => {
        console.log('Session started:', session);
        updateStatus('Session started successfully!', 'success');
        updateSessionButton('End Session');
    });
    
    window.electronAPI.onSessionEnded((session) => {
        console.log('Session ended:', session);
        currentSession = null;
        updateStatus('Session ended', 'info');
        updateSessionButton('Start Session');
        hideTranscriptSection();
    });
    
    window.electronAPI.onTranscriptLine((line) => {
        console.log('Transcript line received:', line);
        addTranscriptLine(line);
    });
}

// Audio event setup
function setupAudioEvents() {
    window.electronAPI.onAudioInitialized(() => {
        console.log('Audio initialized');
        isDeepgramConnected = true;
        updateDeepgramStatus(true);
    });
    
    window.electronAPI.onAudioStarted(() => {
        console.log('Audio capture started');
        isAudioCapturing = true;
        updateAudioButton(true);
    });
    
    window.electronAPI.onAudioStopped(() => {
        console.log('Audio capture stopped');
        isAudioCapturing = false;
        updateAudioButton(false);
    });
    
    window.electronAPI.onAudioError((error) => {
        console.error('Audio error:', error);
        updateStatus('Audio error: ' + error, 'error');
    });
}

// Handler functions for button clicks
async function handleSalesforceAuth() {
    try {
        console.log('Starting Salesforce authentication...');
        updateStatus('Authenticating with Salesforce...', 'info');
        
        const result = await window.electronAPI.authenticateSalesforce();
        console.log('Salesforce result:', result);
        
        if (result.success) {
            updateStatus('Salesforce authenticated!', 'success');
            updateAuthStatus('salesforce', true);
            await checkAuthStatus();
        } else {
            updateStatus('Salesforce auth failed: ' + result.error, 'error');
        }
    } catch (error) {
        console.error('Salesforce auth error:', error);
        updateStatus('Salesforce auth error: ' + error.message, 'error');
    }
}

async function handleSlackAuth() {
    try {
        console.log('Starting Slack authentication...');
        updateStatus('Authenticating with Slack...', 'info');
        
        const result = await window.electronAPI.authenticateSlack();
        console.log('Slack result:', result);
        
        if (result.success) {
            updateStatus('Slack authenticated!', 'success');
            updateAuthStatus('slack', true);
            await checkAuthStatus();
        } else {
            updateStatus('Slack auth failed: ' + result.error, 'error');
        }
    } catch (error) {
        console.error('Slack auth error:', error);
        updateStatus('Slack auth error: ' + error.message, 'error');
    }
}

async function handleStartSession() {
    try {
        if (!isAuthenticated) {
            updateStatus('Please authenticate with Salesforce and Slack first', 'warning');
            return;
        }
        
        console.log('Starting session...');
        updateStatus('Starting session...', 'info');
        
        // Connect to WebSocket if not already connected
        if (!websocketConnected) {
            console.log('Connecting to WebSocket...');
            const connectResult = await window.electronAPI.connectWebSocket();
            if (!connectResult.success) {
                updateStatus('Failed to connect to Heroku: ' + connectResult.error, 'error');
                return;
            }
        }
        
        // Simply trigger session creation - the onSessionCreated event handler will handle the response
        console.log('Requesting session creation...');
        const result = await window.electronAPI.createSession();
        console.log('Session creation request result:', result);
        
        // The actual session handling is done in the onSessionCreated event handler
        // We don't need to do anything else here - onSessionCreated will:
        // 1. Extract the session ID properly
        // 2. Update the UI with "Session created! ID: ..."
        // 3. Show the "End Session" button
        
    } catch (error) {
        console.error('Session error:', error);
        updateStatus('Session error: ' + error.message, 'error');
    }
}

async function handleEndSession() {
    try {
        if (!currentSession) {
            updateStatus('No active session to end', 'warning');
            return;
        }
        
        console.log('Ending session...');
        updateStatus('Ending session...', 'info');
        
        // Handle different session ID structures
        let sessionId = null;
        if (currentSession.session_id) {
            sessionId = currentSession.session_id;
        } else if (currentSession.id) {
            sessionId = currentSession.id;
        } else if (typeof currentSession === 'string') {
            sessionId = currentSession;
        }
        
        if (!sessionId) {
            updateStatus('Cannot find session ID to end', 'error');
            console.error('Current session structure:', currentSession);
            return;
        }
        
        const result = await window.electronAPI.endSession(sessionId);
        console.log('End session result:', result);
        
        if (result.success) {
            updateStatus('Session ended!', 'success');
        } else {
            updateStatus('Failed to end session: ' + result.error, 'error');
        }
    } catch (error) {
        console.error('End session error:', error);
        updateStatus('End session error: ' + error.message, 'error');
    }
}

// Simple authentication functions
async function authenticateSalesforce() {
    try {
        console.log('Starting Salesforce authentication...');
        updateStatus('Authenticating with Salesforce...', 'info');
        
        const result = await window.electronAPI.authenticateSalesforce();
        console.log('Salesforce result:', result);
        
        if (result.success) {
            updateStatus('Salesforce authenticated!', 'success');
            updateAuthStatus('salesforce', true);
            await checkAuthStatus();
        } else {
            updateStatus('Salesforce auth failed: ' + result.error, 'error');
        }
    } catch (error) {
        console.error('Salesforce auth error:', error);
        updateStatus('Salesforce auth error: ' + error.message, 'error');
    }
}

async function authenticateSlack() {
    try {
        console.log('Starting Slack authentication...');
        updateStatus('Authenticating with Slack...', 'info');
        
        const result = await window.electronAPI.authenticateSlack();
        console.log('Slack result:', result);
        
        if (result.success) {
            updateStatus('Slack authenticated!', 'success');
            updateAuthStatus('slack', true);
            await checkAuthStatus();
        } else {
            updateStatus('Slack auth failed: ' + result.error, 'error');
        }
    } catch (error) {
        console.error('Slack auth error:', error);
        updateStatus('Slack auth error: ' + error.message, 'error');
    }
}

// Audio functions
// Audio initialization is now handled by the audio capture button
// This prevents the app from freezing on startup due to microphone permission requests

async function handleStartAudioCapture() {
    try {
        console.log('Start audio capture button clicked');
        updateStatus('Looking for BlackHole audio device...', 'info');
        
        // Get all available audio devices
        const devices = await navigator.mediaDevices.enumerateDevices();
        console.log('DEBUG: Total devices found:', devices.length);
        
        // Log ALL devices for debugging
        console.log('DEBUG: All available devices:');
        devices.forEach((device, index) => {
            console.log(`  ${index}: ${device.kind} - "${device.label}" (ID: ${device.deviceId})`);
        });
        
        // Log only audio input devices
        const audioInputs = devices.filter(d => d.kind === 'audioinput');
        console.log('DEBUG: Audio input devices:', audioInputs.length);
        audioInputs.forEach((device, index) => {
            console.log(`  Input ${index}: "${device.label}"`);
        });
        
        // Look specifically for BlackHole device (following the working system approach)
        const blackHoleDevice = devices.find(device =>
            device.kind === 'audioinput' && 
            device.label.toLowerCase().includes('blackhole')
        );
        
        if (!blackHoleDevice) {
            updateStatus('BlackHole device not found. Please ensure BlackHole is installed and "Astro-Meeting" Multi-Output device is set as system output.', 'error');
            console.log('DEBUG: BlackHole device not found in available devices');
            console.log('DEBUG: Available audio input devices:');
            devices.filter(d => d.kind === 'audioinput').forEach(device => {
                console.log(`- "${device.label}"`);
            });
            
            // Try to find any device with "astro" in the name as fallback
            const astroDevice = devices.find(device =>
                device.kind === 'audioinput' && 
                device.label.toLowerCase().includes('astro')
            );
            
            if (astroDevice) {
                console.log('DEBUG: Found Astro device as fallback:', astroDevice.label);
                updateStatus('BlackHole not found, but found Astro device. Trying that...', 'warning');
                // Continue with astroDevice instead
                const fallbackDevice = astroDevice;
                console.log('Using fallback device:', fallbackDevice.label);
                
                const constraints = {
                    audio: {
                        deviceId: { exact: fallbackDevice.deviceId },
                        sampleRate: 16000,
                        channelCount: 2,  // Match BlackHole stereo
                        echoCancellation: false,
                        noiseSuppression: false,
                        autoGainControl: false
                    },
                    video: false
                };
                
                const systemAudioStream = await navigator.mediaDevices.getUserMedia(constraints);
                console.log('Successfully captured system audio stream from fallback device!');
                
                // Start system audio capture with fallback device
                const systemSuccess = await audioManager.startSystemAudioCapture(systemAudioStream);
                if (!systemSuccess) {
                    updateStatus('Failed to start system audio monitoring with fallback device', 'error');
                    return;
                }
                
                // Start microphone capture
                const micSuccess = await audioManager.startMicrophoneCapture();
                if (!micSuccess) {
                    updateStatus(`System audio working with ${fallbackDevice.label}, but microphone failed`, 'warning');
                } else {
                    console.log('Both fallback system and microphone audio capture started!');
                }
                
                updateStatus(`Dual audio capture started: ${fallbackDevice.label} + Microphone!`, 'success');
                updateAudioButton(true);
                setupAudioLevelIndicator();
                
                // Show Deepgram section when audio is ready
                showSection('deepgram-section');
                return;
            }
            
            return;
        }
        
        console.log('Found BlackHole device:', blackHoleDevice.label);
        updateStatus(`Connecting to: ${blackHoleDevice.label}`, 'info');
        
        // Request access to BlackHole device with 16kHz sample rate (matching working system)
        const constraints = {
            audio: {
                deviceId: { exact: blackHoleDevice.deviceId },
                sampleRate: 16000,  // 16kHz like the working system
                channelCount: 2,    // Stereo (BlackHole is 2ch)
                echoCancellation: false,
                noiseSuppression: false,
                autoGainControl: false
            },
            video: false
        };
        
        const systemAudioStream = await navigator.mediaDevices.getUserMedia(constraints);
        console.log('Successfully captured system audio stream from BlackHole device!');
        console.log('System Stream settings:', {
            sampleRate: systemAudioStream.getAudioTracks()[0].getSettings().sampleRate,
            channelCount: systemAudioStream.getAudioTracks()[0].getSettings().channelCount
        });
        
        // Start system audio capture
        const systemSuccess = await audioManager.startSystemAudioCapture(systemAudioStream);
        if (!systemSuccess) {
            updateStatus('Failed to start system audio monitoring', 'error');
            return;
        }
        
        console.log('System audio capture started, now starting microphone...');
        updateStatus('System audio connected, starting microphone...', 'info');
        
        // Start microphone capture
        const micSuccess = await audioManager.startMicrophoneCapture();
        if (!micSuccess) {
            updateStatus('System audio working, but microphone failed', 'warning');
            // Continue anyway - system audio is still valuable
        } else {
            console.log('Both system and microphone audio capture started!');
        }
        
        updateStatus(`Dual audio capture started: ${blackHoleDevice.label} + Microphone!`, 'success');
        updateAudioButton(true);
        setupAudioLevelIndicator();
        
        // Show Deepgram section when audio is ready
        showSection('deepgram-section');
        
    } catch (error) {
        console.error('Error starting audio capture:', error);
        if (error.name === 'NotAllowedError') {
            updateStatus('Microphone permission denied. Please allow access in System Preferences.', 'error');
        } else if (error.name === 'NotFoundError') {
            updateStatus('BlackHole device not accessible. Please check BlackHole installation and Multi-Output device setup.', 'error');
        } else {
            updateStatus('Audio capture error: ' + error.message, 'error');
        }
    }
}

async function handleStopAudioCapture() {
    try {
        console.log('Stop audio capture button clicked');
        updateStatus('Stopping audio capture...', 'info');
        
        const success = await audioManager.stopAudioCapture();
        console.log('Audio capture stop result:', success);
        
        if (success) {
            updateStatus('Audio capture stopped!', 'success');
            updateAudioButton(false);
        } else {
            updateStatus('Failed to stop audio capture', 'error');
        }
    } catch (error) {
        console.error('Error stopping audio capture:', error);
        updateStatus('Audio capture error: ' + error.message, 'error');
    }
}

// Deepgram handler functions (isolated from existing audio)
async function handleDeepgramConnect() {
    try {
        console.log('Connecting to Deepgram...');
        updateStatus('Connecting to Deepgram...', 'info');
        
        if (!deepgramManager) {
            deepgramManager = new DeepgramManager();
            setupDeepgramEvents();
        }
        
        const success = await deepgramManager.connect();
        if (success) {
            updateStatus('Connected to Deepgram!', 'success');
            updateDeepgramButtons(true, false);
            showSection('deepgram-section');
        } else {
            updateStatus('Failed to connect to Deepgram', 'error');
        }
    } catch (error) {
        console.error('Error connecting to Deepgram:', error);
        updateStatus('Deepgram connection error: ' + error.message, 'error');
    }
}

async function handleDeepgramDisconnect() {
    try {
        console.log('Disconnecting from Deepgram...');
        updateStatus('Disconnecting from Deepgram...', 'info');
        
        if (deepgramManager) {
            const success = await deepgramManager.disconnect();
            if (success) {
                updateStatus('Disconnected from Deepgram', 'info');
                updateDeepgramButtons(false, false);
                updateTranscriptionButtons(false, false);
            }
        }
    } catch (error) {
        console.error('Error disconnecting from Deepgram:', error);
        updateStatus('Deepgram disconnect error: ' + error.message, 'error');
    }
}

async function handleTranscriptionStart() {
    try {
        console.log('Starting transcription...');
        updateStatus('Starting transcription...', 'info');
        
        if (!deepgramManager) {
            updateStatus('Deepgram not connected', 'error');
            return;
        }
        
        // Make sure audio is capturing first
        if (!audioManager || !audioManager.isCapturing) {
            updateStatus('Please start audio capture first', 'warning');
            return;
        }
        
        const success = await deepgramManager.startStreaming();
        if (success) {
            updateStatus('Transcription started!', 'success');
            updateTranscriptionButtons(false, true);
        } else {
            updateStatus('Failed to start transcription', 'error');
        }
    } catch (error) {
        console.error('Error starting transcription:', error);
        updateStatus('Transcription start error: ' + error.message, 'error');
    }
}

async function handleTranscriptionStop() {
    try {
        console.log('Stopping transcription...');
        updateStatus('Stopping transcription...', 'info');
        
        if (deepgramManager) {
            const success = await deepgramManager.stopStreaming();
            if (success) {
                updateStatus('Transcription stopped', 'info');
                updateTranscriptionButtons(true, false);
            }
        }
    } catch (error) {
        console.error('Error stopping transcription:', error);
        updateStatus('Transcription stop error: ' + error.message, 'error');
    }
}

function handleClearTranscript() {
    try {
        console.log('Clearing transcript...');
        if (deepgramManager) {
            deepgramManager.clearTranscripts();
            updateStatus('Transcript cleared', 'info');
        }
    } catch (error) {
        console.error('Error clearing transcript:', error);
        updateStatus('Clear transcript error: ' + error.message, 'error');
    }
}

// Deepgram UI update functions
function updateDeepgramButtons(connected, streaming) {
    const connectBtn = document.getElementById('deepgram-connect-btn');
    const disconnectBtn = document.getElementById('deepgram-disconnect-btn');
    
    if (connectBtn && disconnectBtn) {
        if (connected) {
            connectBtn.style.display = 'none';
            disconnectBtn.style.display = 'inline-flex';
        } else {
            connectBtn.style.display = 'inline-flex';
            disconnectBtn.style.display = 'none';
        }
    }
}

function updateTranscriptionButtons(canStart, isStreaming) {
    const startBtn = document.getElementById('transcription-start-btn');
    const stopBtn = document.getElementById('transcription-stop-btn');
    
    if (startBtn && stopBtn) {
        if (isStreaming) {
            startBtn.style.display = 'none';
            stopBtn.style.display = 'inline-flex';
        } else {
            startBtn.style.display = 'inline-flex';
            stopBtn.style.display = 'none';
        }
        
        startBtn.disabled = !canStart;
        stopBtn.disabled = !isStreaming;
    }
}

// Setup Deepgram event listeners
function setupDeepgramEvents() {
    if (!window.electronAPI) return;
    
    // Listen for Deepgram events from main process
    window.electronAPI.onDeepgramConnected(() => {
        console.log('Deepgram connected event received');
        updateStatus('Deepgram connected!', 'success');
    });
    
    window.electronAPI.onDeepgramDisconnected((event, data) => {
        console.log('Deepgram disconnected event received:', data);
        updateStatus('Deepgram disconnected', 'info');
    });
    
    window.electronAPI.onDeepgramError((event, error) => {
        console.error('Deepgram error event received:', error);
        updateStatus('Deepgram error: ' + error, 'error');
    });
    
    window.electronAPI.onDeepgramTranscript((event, transcript) => {
        console.log('Deepgram transcript received:', transcript);
        if (deepgramManager && transcript.text && transcript.text.trim()) {
            deepgramManager.addTranscript(transcript);
        }
    });
    
    window.electronAPI.onDeepgramStreamingStarted(() => {
        console.log('Deepgram streaming started event received');
        updateStatus('Transcription streaming started', 'success');
        
        // Auto-start audio streaming when Deepgram is ready
        if (deepgramManager && audioManager && audioManager.isCapturing) {
            console.log('Auto-starting audio streaming to Deepgram...');
            deepgramManager.isStreaming = true;
            deepgramManager.startAudioStreaming();
            updateStatus('Audio streaming to Deepgram started!', 'success');
        }
        updateTranscriptionButtons();
    });
    
    window.electronAPI.onDeepgramStreamingStopped(() => {
        console.log('Deepgram streaming stopped event received');
        updateStatus('Transcription streaming stopped', 'info');
    });
}

// Simple session function
async function handleStartSession() {
    try {
        if (!isAuthenticated) {
            updateStatus('Please authenticate with Salesforce and Slack first', 'warning');
            return;
        }
        
        console.log('Starting session...');
        updateStatus('Starting session...', 'info');
        
        // Connect to WebSocket if not connected
        if (!websocketConnected) {
            console.log('Connecting to WebSocket...');
            const wsResult = await window.electronAPI.connectWebSocket();
            if (!wsResult.success) {
                updateStatus('Failed to connect to Heroku: ' + wsResult.error, 'error');
                return;
            }
        }
        
        // Create session
        console.log('Creating session...');
        const createResult = await window.electronAPI.createSession();
        if (!createResult.success) {
            updateStatus('Failed to create session: ' + createResult.error, 'error');
            return;
        }
        
        // This code has been moved to the main handleStartSession function above
        // Remove this duplicate to prevent conflicts
        
    } catch (error) {
        console.error('Session error:', error);
        updateStatus('Session error: ' + error.message, 'error');
    }
}

// Simple status check
async function checkInitialStatus() {
    try {
        console.log('Checking initial status...');
        updateStatus('Ready to start!', 'info');
        
        // Check current auth status
        await checkAuthStatus();
        
    } catch (error) {
        console.error('Status check error:', error);
    }
}

async function checkAuthStatus() {
    try {
        console.log('Checking auth status...');
        const status = await window.electronAPI.checkAuthStatus();
        console.log('Auth status check result:', status);
        
        updateAuthStatus('salesforce', status.salesforce);
        updateAuthStatus('slack', status.slack);
        
        // Update the global authentication status
        isAuthenticated = status.salesforce && status.slack;
        console.log('Both services authenticated:', isAuthenticated);
        
        if (isAuthenticated) {
            console.log('Showing ready section and hiding auth buttons');
            showSection('ready-section');
            hideSection('auth-buttons');
            updateStatus('All services connected! Ready to start sessions.', 'success');
        } else {
            console.log('Showing auth buttons and hiding ready section');
            showSection('auth-buttons');
            hideSection('ready-section');
            updateStatus('Please authenticate with Salesforce and Slack', 'info');
        }
    } catch (error) {
        console.error('Error checking auth status:', error);
        // Don't crash the app, just show auth buttons
        showSection('auth-buttons');
        hideSection('ready-section');
        updateStatus('Error checking authentication status', 'error');
    }
}

// Simple UI helper functions
function updateStatus(message, type = 'info') {
    const statusElement = document.getElementById('status-message');
    if (statusElement) {
        statusElement.textContent = message;
        statusElement.className = `status-message ${type}`;
    }
    console.log('Status update:', message, type);
}

function updateAuthStatus(service, isConnected) {
    const statusElement = document.getElementById(`${service}-status-text`);
    if (statusElement) {
        if (isConnected) {
            statusElement.textContent = '✓ Connected';
            statusElement.className = 'status-text connected';
        } else {
            statusElement.textContent = '✗ Not connected';
            statusElement.className = 'status-text disconnected';
        }
    }
}

function updateAudioStatus(message) {
    const audioStatusElement = document.getElementById('audio-status');
    if (audioStatusElement) {
        audioStatusElement.textContent = message;
    }
}

function updateAudioButton(text) {
    const audioBtn = document.getElementById('audio-capture-btn');
    if (audioBtn) {
        audioBtn.textContent = text;
    }
}

function updateSessionButton(isActive) {
    const startBtn = document.getElementById('start-session-btn');
    const endBtn = document.getElementById('end-session-btn');
    
    if (isActive) {
        if (startBtn) startBtn.classList.add('hidden');
        if (endBtn) endBtn.classList.remove('hidden');
    } else {
        if (startBtn) startBtn.classList.remove('hidden');
        if (endBtn) endBtn.classList.add('hidden');
    }
}

function updateAudioButton(isCapturing) {
    const startBtn = document.getElementById('audio-capture-btn');
    const stopBtn = document.getElementById('audio-stop-btn');
    
    if (isCapturing) {
        if (startBtn) startBtn.classList.add('hidden');
        if (stopBtn) stopBtn.classList.remove('hidden');
    } else {
        if (startBtn) startBtn.classList.remove('hidden');
        if (stopBtn) stopBtn.classList.add('hidden');
    }
}

function updateDeepgramStatus(isConnected) {
    const statusElement = document.getElementById('deepgram-status-text');
    if (statusElement) {
        if (isConnected) {
            statusElement.textContent = '✓ Connected';
            statusElement.className = 'status-text connected';
        } else {
            statusElement.textContent = '✗ Not connected';
            statusElement.className = 'status-text disconnected';
        }
    }
}

function showTranscriptSection() {
    const section = document.getElementById('transcript-section');
    if (section) {
        section.classList.remove('hidden');
    }
}

function hideTranscriptSection() {
    const section = document.getElementById('transcript-section');
    if (section) {
        section.classList.add('hidden');
    }
}

function addTranscriptLine(line) {
    const container = document.getElementById('transcript-content');
    if (container) {
        const lineElement = document.createElement('div');
        lineElement.style.marginBottom = '8px';
        lineElement.style.padding = '4px 8px';
        lineElement.style.backgroundColor = line.isFinal ? '#e8f5e8' : '#fff3e0';
        lineElement.style.borderRadius = '4px';
        lineElement.style.borderLeft = line.isFinal ? '3px solid #28a745' : '3px solid #ffc107';
        
        const confidence = Math.round(line.confidence * 100);
        lineElement.innerHTML = `
            <strong>${line.text}</strong>
            <small style="color: #666; margin-left: 8px;">
                Confidence: ${confidence}% ${line.isFinal ? '(Final)' : '(Interim)'}
            </small>
        `;
        
        container.appendChild(lineElement);
        container.scrollTop = container.scrollHeight;
    }
}

function showSection(sectionId) {
    const section = document.getElementById(sectionId);
    if (section) {
        section.style.display = 'block';
    }
}

function hideSection(sectionId) {
    const section = document.getElementById(sectionId);
    if (section) {
        section.style.display = 'none';
    }
}

function setupAudioLevelIndicator() {
    console.log('Setting up dual audio level indicator...');
    
    // Create audio level display
    const audioSection = document.querySelector('.audio-section') || document.getElementById('audio-controls-section');
    if (audioSection) {
        // Remove existing indicator if present
        const existingIndicator = audioSection.querySelector('.audio-level-indicator');
        if (existingIndicator) {
            existingIndicator.remove();
        }
        
        // Create new dual indicator
        const indicator = document.createElement('div');
        indicator.className = 'audio-level-indicator';
        indicator.innerHTML = `
            <div class="system-audio-section">
                <div class="audio-label">System Audio (BlackHole):</div>
                <div class="audio-level-bar system-bar">
                    <div class="audio-level-fill system-fill" style="width: 0%"></div>
                </div>
                <div class="audio-level-text system-text">Level: 0%</div>
            </div>
            <div class="mic-audio-section">
                <div class="audio-label">Microphone:</div>
                <div class="audio-level-bar mic-bar">
                    <div class="audio-level-fill mic-fill" style="width: 0%"></div>
                </div>
                <div class="audio-level-text mic-text">Level: 0%</div>
            </div>
            <div class="combined-status">
                <div class="audio-signal-status">Combined Signal: No</div>
            </div>
        `;
        indicator.style.cssText = `
            margin-top: 10px;
            padding: 10px;
            background: #f5f5f5;
            border-radius: 5px;
        `;
        
        // Style the sections
        const sections = indicator.querySelectorAll('.system-audio-section, .mic-audio-section');
        sections.forEach(section => {
            section.style.cssText = `
                margin-bottom: 10px;
                padding: 5px;
                background: #fff;
                border-radius: 3px;
            `;
        });
        
        // Style the labels
        const labels = indicator.querySelectorAll('.audio-label');
        labels.forEach(label => {
            label.style.cssText = `
                font-weight: bold;
                margin-bottom: 3px;
                font-size: 12px;
            `;
        });
        
        // Style the level bars
        const levelBars = indicator.querySelectorAll('.audio-level-bar');
        levelBars.forEach(bar => {
            bar.style.cssText = `
                width: 100%;
                height: 15px;
                background: #ddd;
                border-radius: 8px;
                overflow: hidden;
                margin-bottom: 3px;
            `;
        });
        
        // Style the level fills
        const systemFill = indicator.querySelector('.system-fill');
        systemFill.style.cssText = `
            height: 100%;
            background: linear-gradient(90deg, #2196F3, #03A9F4, #00BCD4);
            transition: width 0.1s ease;
            width: 0%;
        `;
        
        const micFill = indicator.querySelector('.mic-fill');
        micFill.style.cssText = `
            height: 100%;
            background: linear-gradient(90deg, #4CAF50, #8BC34A, #CDDC39);
            transition: width 0.1s ease;
            width: 0%;
        `;
        
        // Style the combined status
        const combinedStatus = indicator.querySelector('.combined-status');
        combinedStatus.style.cssText = `
            text-align: center;
            font-weight: bold;
            padding-top: 5px;
            border-top: 1px solid #ddd;
        `;
        
        audioSection.appendChild(indicator);
        
        // Update audio levels periodically
        setInterval(() => {
            const systemLevel = audioManager.systemAudioLevel || 0;
            const micLevel = audioManager.micAudioLevel || 0;
            const combinedLevel = audioManager.getAudioLevel();
            const hasSignal = audioManager.hasAudioSignalDetected();
            
            // Update system audio
            systemFill.style.width = systemLevel + '%';
            indicator.querySelector('.system-text').textContent = `Level: ${systemLevel.toFixed(1)}%`;
            
            // Update microphone
            micFill.style.width = micLevel + '%';
            indicator.querySelector('.mic-text').textContent = `Level: ${micLevel.toFixed(1)}%`;
            
            // Update combined status
            indicator.querySelector('.audio-signal-status').textContent = `Combined Signal: ${hasSignal ? 'Yes' : 'No'}`;
            
            if (combinedLevel > 0) {
                console.log('Dual Audio - System:', systemLevel.toFixed(1) + '%', 'Mic:', micLevel.toFixed(1) + '%', 'Combined Signal:', hasSignal);
            }
        }, 100);
    }
}




